---
title: "한샘몰 Memory Leak, OOM 이슈 분석 및 해결 과정"
publishedAt: "2025-04-04"
updatedAt: "2025-04-04"
summary: ""
---

2024년 말 까지, 기간계파트 담당을 하고 있었기 때문에 한샘몰의 한샘몰의 메모리 누수 및 OOM 이슈는 DevOps 파트에서 전해들어 인지하고는 있었지만 해당 이슈에 대해서 별다른 대응은 하지 않고 있었다. 그런데 25년이 되고 웹 성능 개선 파트를 리딩하게 되면서 몰쪽 코드를 전반적으로 건들게 되었고, 디자인시스템, 전시 컴포넌트 시스템, 한샘몰 코드 베이스까지 모든 C사이드(Customer Side)의 코드를 분석해야 했고, 토스의 이벤트가 트리거가 되어 기존 한샘몰의 메모리 누수 이슈도 부각되며 자연스럽게 업무가 할당되었다. 기존의 기간계 운영까지 모두 넘기고 메모리 누수 이슈에 집중하라는 미션을 받게 되어서 디버깅과 해결에 집중할 수 있는 시간이 주어졌고 이 글은 메모리 누수의 디버깅 과정과 해결 과정을 다루고 있다.

3월 중순 80만 정도의 유입이 이루어진 토스 이벤트에서 모든 서비스가 죽어버리는 일이 생겨버렸고, EKS 환경에서 BE의 Pod는 5대에서 60대, FE의 Pod는 15대에서 60대로 증설했고, FE Pod는 0.5core에서 1core, 4GB에서 6GB 스케일업 했지만, 서비스가 중단되는 현상이 발생했다. 그리고 IDC(기간계)쪽과 AWS간의 통신 대역폭(500Mbps)이 모두 차버리는 현상으로 용량을 임시 증설해야 하는 상황까지 이어졌다.

이 상황에서 FE의 메모리 누수 이슈는 이전 부터 DevOps와 C파트 리더가 원인 분석을 진행하고 있었지만, 누적 되어 Pod가 재기동되는 현상이 있었지만 Pod의 재기동으로 일상적인 서비스의 유지에는 별다른 이슈가 없었기 때문에 지지부진한 상황이었던 것 같다. 그러나 이벤트가 발생하며 대규모 트래픽이 한꺼번에 들어오면서 메모리의 누수 속도가 가속화되었고 Pod가 재기동되는 속도보다 죽는 속도가 더 빨라서 서버가 죽는 상황이 발생했고, 메모리 누수가 회사 차원에서 부각되었다.

우선 메모리 누수 현상을 파악하기 위해 CloudWatch의 메모리 사용률부터 분석했다. 언제부터 누수 현상이 발생했고, 어떤 그래프 모양을 그리고 있는지 파악해야 어느 지점을 분석해야 할지 알 수 있을 것이라고 생각했다.

![image](/assets/202504/001.png)

![image](/assets/202504/002.png)

그래프의 모양은 전형적인 메모리 누수의 형태를 띄고 있었다. 우상향하며 메모리 사용율이 증가했고, GC가 이루어지기 전에 OOM Kill이 발생하며, Pod가 재기동되며, 메모리의 사용률이 다시 떨어지는 현상이 발생했다.